{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'D:\\Legal Clause Extraction\\cuad\\data\\CUADv1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf95c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the minified JSON\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Save it pretty-printed\n",
    "with open(\"cuad_data_pretty.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237271e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the minified JSON\n",
    "with open(r'D:\\Legal Clause Extraction\\cuad\\data\\test.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Save it pretty-printed\n",
    "with open(\"cuad_test_pretty.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72512820",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CATEGORIES = [\n",
    "    \"Agreement Date\",\n",
    "    \"Effective Date\",\n",
    "    \"Expiration Date\",\n",
    "    \"Renewal Term\",\n",
    "    \"Notice Period to Terminate Renewal\",\n",
    "    \"Parties\",\n",
    "    \"Governing Law\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb25173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the categories you want to keep\n",
    "TARGET_CATEGORIES = {\n",
    "    \"Agreement Date\",\n",
    "    \"Effective Date\",\n",
    "    \"Expiration Date\",\n",
    "    \"Renewal Term\",\n",
    "    \"Notice Period to Terminate Renewal\",\n",
    "    \"Parties\",\n",
    "    \"Governing Law\"\n",
    "}\n",
    "\n",
    "def filter_cuad_file(input_path, output_path, categories_to_keep):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filtered_data = {\"data\": []}\n",
    "    \n",
    "    for doc in data[\"data\"]:\n",
    "        new_doc = {\"title\": doc[\"title\"], \"paragraphs\": []}\n",
    "        for para in doc[\"paragraphs\"]:\n",
    "            new_qas = [\n",
    "                qa for qa in para[\"qas\"]\n",
    "                if any(qa[\"id\"].endswith(\"__\" + cat) for cat in categories_to_keep)\n",
    "            ]\n",
    "            if new_qas:\n",
    "                new_doc[\"paragraphs\"].append({\n",
    "                    \"context\": para.get(\"context\", \"\"),  \n",
    "                    \"qas\": new_qas\n",
    "                })\n",
    "        if new_doc[\"paragraphs\"]:\n",
    "            filtered_data[\"data\"].append(new_doc)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "filter_cuad_file('D:\\Legal Clause Extraction\\cuad_data_pretty.json', \"train_filtered.json\", TARGET_CATEGORIES)\n",
    "filter_cuad_file('D:\\Legal Clause Extraction\\cuad_test_pretty.json', \"test_filtered.json\", TARGET_CATEGORIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d4f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_context(text):\n",
    "    text = text.replace('\\r\\n', '\\n')  # Windows line endings\n",
    "    text = text.replace('\\r', '\\n')    # Mac-style line endings\n",
    "    text = re.sub(r'\\u2028|\\u2029', '\\n', text)  # Unicode line separators\n",
    "    text = re.sub(r'\\x00', '', text)   # Remove null characters\n",
    "    text = re.sub(r'[ \\t]{2,}', ' ', text)  # Compress long spaces\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Collapse too many newlines\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "#  CONFIG: Set your file paths\n",
    "train_input_path = \"train_filtered.json\"\n",
    "test_input_path = \"test_filtered.json\"\n",
    "train_output_path = \"train_flat.json\"\n",
    "test_output_path = \"test_flat.json\"\n",
    "\n",
    "def flatten_cuad(input_path, output_path, drop_impossible=False):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    flat = []\n",
    "\n",
    "    for doc in data[\"data\"]:\n",
    "        title = doc.get(\"title\", \"\")\n",
    "        for para in doc.get(\"paragraphs\", []):\n",
    "            # Determine context (CUAD may not have it separately)\n",
    "            context = para.get(\"context\")\n",
    "            if context is None:\n",
    "                # If no explicit context, assume full contract is stored under 'context' or part of outer structure\n",
    "                # If paragraphs wrap chunks, you might need to inject full contract text here\n",
    "                context = \"\"  # Placeholder (you can customize)\n",
    "            for qa in para.get(\"qas\", []):\n",
    "                if drop_impossible and qa.get(\"is_impossible\", False):\n",
    "                    continue\n",
    "                flat.append({\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"context\": clean_context(context),\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answers\": qa[\"answers\"],\n",
    "                    \"is_impossible\": qa.get(\"is_impossible\", False)\n",
    "                })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flat, f, indent=2,ensure_ascii=False)\n",
    "\n",
    "    print(f\" Flattened {len(flat)} QA pairs -> {output_path}\")\n",
    "\n",
    "# üîß Run on both train and test sets\n",
    "flatten_cuad(train_input_path, train_output_path, drop_impossible=False)\n",
    "flatten_cuad(test_input_path, test_output_path, drop_impossible=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_flat.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0][\"context\"])  # If this prints with proper line breaks, your file is fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ff1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Load API key\n",
    "with open(\"api.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# 2. Load flattened QA data\n",
    "with open(\"train_flat.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"test_flat.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# 3. Configuration\n",
    "NUM_FEW_SHOT     = 4\n",
    "CONTRACT_ID      = \"CENTRACKINTERNATIONALINC_10_29_1999-EX-10.3-WEB SITE HOSTING AGREEMENT\"\n",
    "CHUNK_TOKENS     = 1500\n",
    "MAX_TOTAL_TOKENS = 4000\n",
    "MAX_OUTPUT_TOKENS= 100\n",
    "TEMPERATURE      = 0.2\n",
    "\n",
    "# 4. Tokenizer for GPT-3.5-turbo\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "def num_tokens(text: str) -> int:\n",
    "    return len(enc.encode(text))\n",
    "def chunk_text_by_tokens(text: str, max_tokens: int):\n",
    "    toks = enc.encode(text)\n",
    "    return [enc.decode(toks[i : i + max_tokens]) for i in range(0, len(toks), max_tokens)]\n",
    "\n",
    "# 5. System prompt to enforce span-only responses\n",
    "SYSTEM = (\n",
    "    \"You are a legal assistant. When I give you a contract and question, \"\n",
    "    \"extract and return only the exact text span from the contract that answers the question. \"\n",
    "    \"Do not explain, rephrase, or add any extra text.\"\n",
    ")\n",
    "\n",
    "# 6. Prepare few-shot examples **with** answers\n",
    "def format_fs_example(e):\n",
    "    ans = e['answers'][0]['text'].strip() if e['answers'] else '[No answer]'\n",
    "    return f\"\"\"Contract:\n",
    "{e['context'].strip()}\n",
    "\n",
    "Question:\n",
    "{e['question'].strip()}\n",
    "\n",
    "Answer:\n",
    "{ans}\"\"\"\n",
    "\n",
    "few_shot = \"\\n\\n---\\n\\n\".join(format_fs_example(e) for e in train_data[:NUM_FEW_SHOT])\n",
    "fs_tokens = num_tokens(few_shot)\n",
    "print(f\"üîç Few-shot uses {fs_tokens} tokens\")\n",
    "\n",
    "# 7. Verify available contract prefixes\n",
    "all_prefixes = sorted({e[\"id\"].rsplit(\"__\",1)[0] for e in test_data})\n",
    "print(\"üóÇ Available prefixes:\")\n",
    "for p in all_prefixes:\n",
    "    print(\" \", p)\n",
    "\n",
    "# 8. Filter test entries for our CONTRACT_ID\n",
    "target = [e for e in test_data if e[\"id\"].startswith(CONTRACT_ID)]\n",
    "if not target:\n",
    "    raise ValueError(f\"No entries found for CONTRACT_ID={CONTRACT_ID!r}\")\n",
    "print(f\"\\nüß™ Found {len(target)} questions for:\\n  {CONTRACT_ID}\\n\")\n",
    "\n",
    "# 9. Loop through each clause question\n",
    "for idx, e in enumerate(target, 1):\n",
    "    question = e[\"question\"].strip()\n",
    "    true_ans = e[\"answers\"][0][\"text\"].strip() if e[\"answers\"] else \"[No answer]\"\n",
    "    context = e[\"context\"]\n",
    "    \n",
    "    # If whole context fits, use it; otherwise chunk\n",
    "    if fs_tokens + num_tokens(context) + MAX_OUTPUT_TOKENS + 50 <= MAX_TOTAL_TOKENS:\n",
    "        chunks = [context]\n",
    "    else:\n",
    "        chunks = chunk_text_by_tokens(context, CHUNK_TOKENS)\n",
    "\n",
    "    print(f\"---\\nClause {idx}/{len(target)} ‚Üí {e['id'].split('__')[-1]}\")\n",
    "    print(f\"Q: {question}\")\n",
    "\n",
    "    prediction = \"\"\n",
    "    for c_i, chunk in enumerate(chunks, 1):\n",
    "        # Skip if still too large\n",
    "        if fs_tokens + num_tokens(chunk) + MAX_OUTPUT_TOKENS + 50 > MAX_TOTAL_TOKENS:\n",
    "            continue\n",
    "\n",
    "        prompt = f\"{few_shot}\\n\\n---\\n\\nContract Chunk:\\n{chunk.strip()}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":SYSTEM},\n",
    "                {\"role\":\"user\",\"content\":prompt}\n",
    "            ],\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_OUTPUT_TOKENS\n",
    "        )\n",
    "        ans = resp.choices[0].message.content.strip()\n",
    "        print(f\"  Chunk {c_i}: {ans}\")\n",
    "\n",
    "        if ans and ans.lower() not in {\"no\",\"none\",\"n/a\",\"\"}:\n",
    "            prediction = ans\n",
    "            break\n",
    "\n",
    "    print(f\"\\n GPT:  {prediction}\")\n",
    "    print(f\" True: {true_ans}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "# ===  Load API key ===\n",
    "with open(\"api.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# ===  Load train and test ===\n",
    "with open(\"train_flat.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"test_flat.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "\n",
    "MODEL             = \"gpt-3.5-turbo\"\n",
    "MAX_PROMPT_TOKENS = 4000\n",
    "MAX_OUTPUT_TOKENS = 100\n",
    "TEMPERATURE       = 0.2\n",
    "\n",
    "# ===  Token ===\n",
    "enc = tiktoken.encoding_for_model(MODEL)\n",
    "def num_tokens(text: str) -> int:\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def chunk_text_by_tokens(text: str, max_toks: int):\n",
    "    toks = enc.encode(text)\n",
    "    return [enc.decode(toks[i:i+max_toks]) for i in range(0, len(toks), max_toks)]\n",
    "\n",
    "# ===  System prompt ===\n",
    "SYSTEM = (\n",
    "    \"You are a legal assistant. Extract and return only the exact text span \"\n",
    "    \"from the contract that answers the question. Do not explain, rephrase, or add extra text.\"\n",
    ")\n",
    "\n",
    "\n",
    "def build_fewshot(train_data, clause_types, max_tokens=1000):\n",
    "    fewshot = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for entry in train_data:\n",
    "        clause = entry[\"id\"].split(\"__\")[-1]\n",
    "        if clause not in clause_types:\n",
    "            continue\n",
    "        context = entry[\"context\"].strip()\n",
    "        question = entry[\"question\"].strip()\n",
    "        answers = entry[\"answers\"]\n",
    "        if not answers:\n",
    "            continue\n",
    "        answer_text = answers[0][\"text\"].strip()\n",
    "\n",
    "        # Truncate context around first answer (¬±300 chars window)\n",
    "        ans_start = answers[0][\"answer_start\"]\n",
    "        snippet = context[max(0, ans_start - 300): ans_start + 300].replace(\"\\n\", \" \")\n",
    "\n",
    "        example = f\"\"\"Contract:\n",
    "{snippet}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer_text}\"\"\"\n",
    "        tokens = num_tokens(example)\n",
    "        if total_tokens + tokens > max_tokens:\n",
    "            break\n",
    "        fewshot.append(example)\n",
    "        total_tokens += tokens\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(fewshot), total_tokens\n",
    "\n",
    "# ===  Pick a test case ===\n",
    "CONTRACT_ID = \"CENTRACKINTERNATIONALINC_10_29_1999-EX-10.3-WEB SITE HOSTING AGREEMENT\"\n",
    "target = [e for e in test_data if e[\"id\"].startswith(CONTRACT_ID)]\n",
    "if not target:\n",
    "    raise ValueError(f\"No test entries found for {CONTRACT_ID}\")\n",
    "print(f\" Loaded {len(target)} questions from test set\")\n",
    "\n",
    "# ===  Build few-shot from train only (no leakage) ===\n",
    "CLAUSE_TYPES = list({e['id'].split(\"__\")[-1] for e in target})\n",
    "few_shot, fs_tokens = build_fewshot(train_data, CLAUSE_TYPES, max_tokens=1000)\n",
    "print(f\" Few-shot block uses {fs_tokens} tokens across {few_shot.count('Contract:')} examples\\n\")\n",
    "\n",
    "\n",
    "for idx, entry in enumerate(target, 1):\n",
    "    question = entry[\"question\"].strip()\n",
    "    true_ans = entry[\"answers\"][0][\"text\"].strip() if entry[\"answers\"] else \"[No answer]\"\n",
    "    context = entry[\"context\"]\n",
    "\n",
    "    chunks = [context] if fs_tokens + num_tokens(context) + MAX_OUTPUT_TOKENS < MAX_PROMPT_TOKENS else chunk_text_by_tokens(context, 1500)\n",
    "\n",
    "    print(f\"---\\nClause {idx}/{len(target)} ‚Üí {entry['id'].split('__')[-1]}\")\n",
    "    print(f\"Q: {question}\")\n",
    "\n",
    "    best = \"\"\n",
    "    for c_i, chunk in enumerate(chunks, 1):\n",
    "        if fs_tokens + num_tokens(chunk) + MAX_OUTPUT_TOKENS > MAX_PROMPT_TOKENS:\n",
    "            continue\n",
    "        prompt = (\n",
    "            f\"{few_shot}\\n\\n---\\n\\n\"\n",
    "            f\"Contract:\\n{chunk.strip()}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=TEMPERATURE,\n",
    "                max_tokens=MAX_OUTPUT_TOKENS\n",
    "            )\n",
    "            ans = resp.choices[0].message.content.strip()\n",
    "            print(f\"  Chunk {c_i} ‚Üí {repr(ans)}\")\n",
    "            if ans and ans.lower() not in {\"none\", \"n/a\", \"\"}:\n",
    "                best = ans\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\" Error: {e}\")\n",
    "\n",
    "    print(f\"\\n GPT: {best}\")\n",
    "    print(f\"True:{true_ans}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_cuad_to_qa_format(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    hf_qa_data = []\n",
    "    skipped = 0\n",
    "\n",
    "    for entry in raw_data:\n",
    "        if not entry[\"answers\"]:\n",
    "            skipped += 1\n",
    "            continue  # Skip entries with no labeled answer\n",
    "\n",
    "        # CUAD format: answers: [{\"text\": ..., \"answer_start\": ...}]\n",
    "        first_answer = entry[\"answers\"][0]\n",
    "        hf_entry = {\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"context\": entry[\"context\"],\n",
    "            \"question\": entry[\"question\"],\n",
    "            \"answers\": {\n",
    "                \"text\": [first_answer[\"text\"]],\n",
    "                \"answer_start\": [first_answer[\"answer_start\"]]\n",
    "            }\n",
    "        }\n",
    "        hf_qa_data.append(hf_entry)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(hf_qa_data, f, indent=2)\n",
    "\n",
    "    print(f\" Converted {len(hf_qa_data)} examples. Skipped {skipped} unlabeled entries.\")\n",
    "    print(f\" Saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "convert_cuad_to_qa_format(\"train_flat.json\", \"cuad_qa_train.json\")\n",
    "convert_cuad_to_qa_format(\"test_flat.json\", \"cuad_qa_test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bde04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\"\n",
    "\n",
    "\n",
    "# === Load your converted dataset ===\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"cuad_qa_train.json\",\n",
    "    \"validation\": \"cuad_qa_test.json\"\n",
    "})\n",
    "\n",
    "# === TEMP: Subset for speed during testing ===\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(300))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(100))\n",
    "\n",
    "# === Load tokenizer + model ===\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# === Preprocessing ===\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# === Align start/end token positions ===\n",
    "def add_token_positions(example):\n",
    "    start_char = example[\"answers\"][\"answer_start\"][0]\n",
    "    end_char = start_char + len(example[\"answers\"][\"text\"][0])\n",
    "    offsets = example[\"offset_mapping\"]\n",
    "\n",
    "    start_token = end_token = 0\n",
    "    for idx, (s, e) in enumerate(offsets):\n",
    "        if s <= start_char < e:\n",
    "            start_token = idx\n",
    "        if s < end_char <= e:\n",
    "            end_token = idx\n",
    "            break\n",
    "\n",
    "    example[\"start_positions\"] = start_token\n",
    "    example[\"end_positions\"] = end_token\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(add_token_positions, remove_columns=[\"offset_mapping\"])\n",
    "\n",
    "# === Training arguments ===\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./cuad_roberta\",\n",
    "    eval_strategy=\"epoch\",                    # use eval_strategy instead of evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,            # simulate larger batches\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300,                            # optional cap for speed during testing\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    fp16=torch.cuda.is_available(),           # enable automatic mixed precision on GPU\n",
    ")\n",
    "\n",
    "# === Optional: Evaluation metrics (disabled here for speed)\n",
    "# squad_metric = evaluate.load(\"squad\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     return squad_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# === Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None  # use compute_metrics=squad_metric if needed\n",
    ")\n",
    "\n",
    "# === Train! ===\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486cf25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
